{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc577887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba64689",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RXZlbnRJbmZvcm1hdGlvbjo0OWQ4M2I3MS1kNDZmLTRiYjMtODJkYy1kMTE0MDg4OTBkOGQ=.json','r') as f:\n",
    "         data = json.loads(f.read())\n",
    "         # Flatten data\n",
    "         data1= pd.json_normalize(data)\n",
    "         data2= pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in glob.glob('C:/Users/chandra.gorripat/Desktop/all/refine data/*[0-9][0-9][0-9]'):\n",
    "    json_files = glob.glob(os.path.join(i, \"*.json\"))\n",
    "    csv_files = glob.glob(os.path.join(i, \"*.csv\"))\n",
    "    print(i)\n",
    "    for f in json_files:\n",
    "        #print(f)\n",
    "        t=f[:-4]+'csv'\n",
    "        #print(t)\n",
    "        data1df1=pd.read_csv(t)\n",
    "        #print(df1)\n",
    "        with open(f,'r') as f:\n",
    "         data1data = json.loads(f.read())\n",
    "         # Flatten data\n",
    "         data1df2 = pd.json_normalize(data1data)\n",
    "        #*********************************************************************************\n",
    "        data1df2['min_sum']=data1df1['sum'].min()\n",
    "        data1df2['max_sum']=data1df1['sum'].max()\n",
    "        data1df2['mean_sum']=data1df1['sum'].mean()\n",
    "        data1df2['std_sum']=data1df1['sum'].std()\n",
    "        data1df2['duration_of_event']=data1df1['timestamp'].max()\n",
    "        #shits**************************************\n",
    "        data1df1['max_l']=0\n",
    "        data1df1['min_l']=0\n",
    "        for i in range(data1df1.shape[0]):\n",
    "            a=data1df1['lc0'][i]\n",
    "            b=data1df1['lc1'][i]\n",
    "            c=data1df1['lc2'][i]\n",
    "            d=data1df1['lc3'][i]\n",
    "            if max(a,b,c,d)==a:\n",
    "                data1df1['max_l'][i]=1\n",
    "            if max(a,b,c,d)==b:\n",
    "                data1df1['max_l'][i]=2\n",
    "            if max(a,b,c,d)==c:\n",
    "                data1df1['max_l'][i]=3    \n",
    "            if max(a,b,c,d)==d:\n",
    "                data1df1['max_l'][i]=4\n",
    "            if min(a,b,c,d)==a:\n",
    "                data1df1['min_l'][i]=1\n",
    "            if min(a,b,c,d)==b:\n",
    "                data1df1['min_l'][i]=2\n",
    "            if min(a,b,c,d)==c:\n",
    "                data1df1['min_l'][i]=3    \n",
    "            if min(a,b,c,d)==d:\n",
    "                data1df1['min_l'][i]=4 \n",
    "                \n",
    "        \n",
    "        count_max_shift=0\n",
    "        count_max_shift1=0\n",
    "        count_max_shift2=0\n",
    "        count_max_shift3=0\n",
    "        count_min_shift=0\n",
    "        count_min_shift1=0\n",
    "        count_min_shift2=0\n",
    "        count_min_shift3=0\n",
    "        interval1=data1df1.shape[0]//3+1\n",
    "        interval2=2*(data1df1.shape[0]//3)+1\n",
    "\n",
    "        for i in range(1,data1df1.shape[0]):\n",
    "            if data1df1['max_l'][i]!=data1df1['max_l'][i-1]:\n",
    "                 count_max_shift=count_max_shift+1\n",
    "                 if i<interval1:\n",
    "                    count_max_shift1=count_max_shift1+1\n",
    "                 elif i>=interval1 and i<interval2:\n",
    "                    count_max_shift2=count_max_shift2+1\n",
    "                 else:\n",
    "                    count_max_shift3=count_max_shift3+1\n",
    "            if data1df1['min_l'][i]!=data1df1['min_l'][i-1]:\n",
    "                 count_min_shift=count_min_shift+1 \n",
    "                 if i<interval1:\n",
    "                    count_min_shift1=count_min_shift1+1\n",
    "                 elif i>=interval1 and i<interval2:\n",
    "                    count_min_shift2=count_min_shift2+1\n",
    "                 else:\n",
    "                    count_min_shift3=count_min_shift3+1\n",
    "                    \n",
    "        #**********************************************************************************\n",
    "        data1df2['count_max_shift']=count_max_shift\n",
    "        data1df2['count_max_shift_1']=count_max_shift1\n",
    "        data1df2['count_max_shift_2']=count_max_shift2\n",
    "        data1df2['count_max_shift_3']=count_max_shift3\n",
    "        \n",
    "        data1df2['count_min_shift']=count_min_shift\n",
    "        data1df2['count_min_shift_1']=count_min_shift1\n",
    "        data1df2['count_min_shift_2']=count_min_shift2\n",
    "        data1df2['count_min_shift_3']=count_min_shift3\n",
    "        data1=data1.append(data1df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb26a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724fd27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in glob.glob('C:/Users/chandra.gorripat/Desktop/all/refine data/*[0-9][0-9][0-9]'):\n",
    "    json_files = glob.glob(os.path.join(i, \"*.json\"))\n",
    "    csv_files = glob.glob(os.path.join(i, \"*.csv\"))\n",
    "    print(i)\n",
    "    for f in json_files:\n",
    "        #print(f)\n",
    "        t=f[:-4]+'csv'\n",
    "        #print(t)\n",
    "        data2df1=pd.read_csv(t)\n",
    "        #print(df1)\n",
    "        with open(f,'r') as f:\n",
    "         data2data = json.loads(f.read())\n",
    "         # Flatten data\n",
    "         data2df2 = pd.json_normalize(data2data)\n",
    "        #*********************************************************************************\n",
    "        data2df2['min_sum']=data2df1['sum'].min()\n",
    "        data2df2['max_sum']=data2df1['sum'].max()\n",
    "        data2df2['mean_sum']=data2df1['sum'].mean()\n",
    "        data2df2['std_sum']=data2df1['sum'].std()\n",
    "        data2df2['duration_of_event']=data2df1['timestamp'].max()\n",
    "        ########################################\n",
    "        \n",
    "        data2df1['new_sum']=(data2df1['sum']//10)*10\n",
    "        arr=data2df1['new_sum'].unique()\n",
    "        element,count=arr[0],data2df1[data2df1['new_sum']==arr[0]].shape[0]\n",
    "        for i in range(1,len(arr)):\n",
    "              tc=data2df1[data2df1['new_sum']==arr[i]].shape[0]\n",
    "              if tc>count:\n",
    "                element=arr[i]\n",
    "                count=tc \n",
    "        \n",
    "        interval1=data2df1.shape[0]//3+1\n",
    "        interval2=2*(data2df1.shape[0]//3)+1\n",
    "        data2df11=data2df1[0:interval1]\n",
    "        data2df12=data2df1[interval1:interval2]\n",
    "        data2df13=data2df1[interval2:]\n",
    "        \n",
    "        arr1=data2df11['new_sum'].unique()\n",
    "        element1,count1=arr1[0],data2df11[data2df11['new_sum']==arr1[0]].shape[0]\n",
    "        for i in range(1,len(arr1)):\n",
    "              tc1=data2df11[data2df11['new_sum']==arr1[i]].shape[0]\n",
    "              if tc1>count1:\n",
    "                element1=arr1[i]\n",
    "                count1=tc1 \n",
    "        \n",
    "        arr2=data2df12['new_sum'].unique()\n",
    "        element2,count2=arr2[0],data2df12[data2df12['new_sum']==arr2[0]].shape[0]\n",
    "        for i in range(1,len(arr2)):\n",
    "              tc2=data2df12[data2df12['new_sum']==arr2[i]].shape[0]\n",
    "              if tc2>count2:\n",
    "                element2=arr2[i]\n",
    "                count2=tc2 \n",
    "        \n",
    "        arr3=data2df13['new_sum'].unique()\n",
    "        element3,count3=arr3[0],data2df13[data2df13['new_sum']==arr3[0]].shape[0]\n",
    "        for i in range(1,len(arr3)):\n",
    "              tc3=data2df13[data2df13['new_sum']==arr3[i]].shape[0]\n",
    "              if tc3>count3:\n",
    "                element3=arr3[i]\n",
    "                count3=tc3        \n",
    "        \n",
    "        data2df2['min_sum_1']=data2df11['sum'].min()\n",
    "        data2df2['max_sum_1']=data2df11['sum'].max()\n",
    "        data2df2['mean_sum_1']=data2df11['sum'].mean()\n",
    "        data2df2['std_sum_1']=data2df11['sum'].std()\n",
    "        \n",
    "        data2df2['min_sum_2']=data2df12['sum'].min()\n",
    "        data2df2['max_sum_2']=data2df12['sum'].max()\n",
    "        data2df2['mean_sum_2']=data2df12['sum'].mean()\n",
    "        data2df2['std_sum_2']=data2df12['sum'].std()\n",
    "        \n",
    "        data2df2['min_sum_3']=data2df13['sum'].min()\n",
    "        data2df2['max_sum_3']=data2df13['sum'].max()\n",
    "        data2df2['mean_sum_3']=data2df13['sum'].mean()\n",
    "        data2df2['std_sum_3']=data2df13['sum'].std()\n",
    "        \n",
    "        data2df2['max_10_dev']=element \n",
    "        data2df2['max_10_dev_1']=element1\n",
    "        data2df2['max_10_dev_2']=element2\n",
    "        data2df2['max_10_dev_3']=element3\n",
    "        data2df2['max_10_dur']=count/40\n",
    "        data2df2['max_10_dur_1']=count1/40\n",
    "        data2df2['max_10_dur_2']=count2/40\n",
    "        data2df2['max_10_dur_3']=count3/40\n",
    "\n",
    "        data2=data2.append(data2df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6dd6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.reset_index(inplace=True)\n",
    "data2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4395981",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop(index=0,inplace=True)\n",
    "data2.drop(index=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf2ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.reset_index(inplace=True)\n",
    "data2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop(['index','level_0'],axis=1,inplace=True)\n",
    "data2.drop(['index','level_0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901278c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop(['Free_text','Tags','Name_of_cat','Weight_of_cat','min_sum','max_sum','mean_sum','std_sum','duration_of_event'],axis=1,inplace=True)\n",
    "data2.drop(['Device_ID','Event_start_time','Free_text','Tare_weight'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefcf923",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(data1,data2, how ='left', on ='Event_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a558be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc382c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    df['Event_start_time'][i]=df['Event_start_time'][i][2:-6]\n",
    "    df['Event_start_time'][i]=df['Event_start_time'][i].replace('-','/')\n",
    "    df['Event_start_time'][i]=df['Event_start_time'][i].replace('T',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "        datetime_str =df['Event_start_time'][i] \n",
    "        df['Event_start_time'][i] = datetime.strptime(datetime_str, '%y/%m/%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77628e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_Tare_weight']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ba01f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    df['Total_Tare_weight'][i]=df['Tare_weight'][i][0]+df['Tare_weight'][i][1]+df['Tare_weight'][i][2]+df['Tare_weight'][i][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ad11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour_day']=0\n",
    "for i in range(df.shape[0]):\n",
    "    df['hour_day'][i]=df['Event_start_time'][i].hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca44ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    df['Weight_of_cat'][i]=int(df['Weight_of_cat'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Weight_of_cat']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f5216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(index=2823,inplace=True)\n",
    "df.drop(index=2950,inplace=True)\n",
    "df.drop(index=10997,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d9096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['index'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492c65da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ratio_10_weight']=(df['Total_Tare_weight']-df['max_10_dev'])/df['Weight_of_cat']\n",
    "df['ratio_10_1_weight']=(df['Total_Tare_weight']-df['max_10_dev_1'])/df['Weight_of_cat']\n",
    "df['ratio_10_2_weight']=(df['Total_Tare_weight']-df['max_10_dev_2'])/df['Weight_of_cat']\n",
    "df['ratio_10_3_weight']=(df['Total_Tare_weight']-df['max_10_dev_3'])/df['Weight_of_cat']\n",
    "\n",
    "df['ratio_min_weight']=(df['Total_Tare_weight']-df['min_sum'])/df['Weight_of_cat']\n",
    "df['ratio_min_1_weight']=(df['Total_Tare_weight']-df['min_sum_1'])/df['Weight_of_cat']\n",
    "df['ratio_min_2_weight']=(df['Total_Tare_weight']-df['min_sum_2'])/df['Weight_of_cat']\n",
    "df['ratio_min_3_weight']=(df['Total_Tare_weight']-df['min_sum_3'])/df['Weight_of_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_10_dur_ratio']=df['max_10_dur']/df['duration_of_event']\n",
    "df['max_10_dur_1_ratio']=3*(df['max_10_dur_1']/df['duration_of_event'])\n",
    "df['max_10_dur_2_ratio']=3*(df['max_10_dur_2']/df['duration_of_event'])\n",
    "df['max_10_dur_3_ratio']=3*(df['max_10_dur_3']/df['duration_of_event'])\n",
    "\n",
    "df['count_max_shift_freq']=df['count_max_shift']/df['duration_of_event']\n",
    "df['count_max_shift_1_freq']=3*(df['count_max_shift_1']/df['duration_of_event'])\n",
    "df['count_max_shift_2_freq']=3*(df['count_max_shift_2']/df['duration_of_event'])\n",
    "df['count_max_shift_3_freq']=3*(df['count_max_shift_3']/df['duration_of_event'])\n",
    "\n",
    "df['count_min_shift_freq']=df['count_max_shift']/df['duration_of_event']\n",
    "df['count_min_shift_1_freq']=3*(df['count_min_shift_1']/df['duration_of_event'])\n",
    "df['count_min_shift_2_freq']=3*(df['count_min_shift_2']/df['duration_of_event'])\n",
    "df['count_min_shift_3_freq']=3*(df['count_min_shift_3']/df['duration_of_event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f9abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['std_sum']=df['std_sum']/df['Total_Tare_weight']\n",
    "df['std_sum_1']=df['std_sum_1']/df['Total_Tare_weight']\n",
    "df['std_sum_2']=df['std_sum_2']/df['Total_Tare_weight']\n",
    "df['std_sum_3']=df['std_sum_3']/df['Total_Tare_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['min_sum']=(df['min_sum']/df['Total_Tare_weight'])\n",
    "df['max_sum']=(df['max_sum']/df['Total_Tare_weight'])\n",
    "df['mean_sum']=(df['mean_sum']/df['Total_Tare_weight'])\n",
    "df['max_10_dev']=(df['max_10_dev']/df['Total_Tare_weight'])\n",
    "\n",
    "df['min_sum_1']=(df['min_sum_1']/df['Total_Tare_weight'])\n",
    "df['max_sum_1']=(df['max_sum_1']/df['Total_Tare_weight'])\n",
    "df['mean_sum_1']=(df['mean_sum_1']/df['Total_Tare_weight'])\n",
    "df['max_10_dev_1']=(df['max_10_dev_1']/df['Total_Tare_weight'])\n",
    "\n",
    "df['min_sum_2']=(df['min_sum_2']/df['Total_Tare_weight'])\n",
    "df['max_sum_2']=(df['max_sum_2']/df['Total_Tare_weight'])\n",
    "df['mean_sum_2']=(df['mean_sum_2']/df['Total_Tare_weight'])\n",
    "df['max_10_dev_2']=(df['max_10_dev_2']/df['Total_Tare_weight'])\n",
    "\n",
    "df['min_sum_3']=(df['min_sum_3']/df['Total_Tare_weight'])\n",
    "df['max_sum_3']=(df['max_sum_3']/df['Total_Tare_weight'])\n",
    "df['mean_sum_3']=(df['mean_sum_3']/df['Total_Tare_weight'])\n",
    "df['max_10_dev_3']=(df['max_10_dev_3']/df['Total_Tare_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0fe90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['elimination']=0\n",
    "for i in range(df.shape[0]):\n",
    "    if 'urination' in df['Tags'][i]:\n",
    "        df['elimination'][i]=1\n",
    "    if 'defecation' in df['Tags'][i]:\n",
    "        df['elimination'][i]=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75065d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4857c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
